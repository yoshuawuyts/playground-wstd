// Generated by `wit-bindgen` 0.25.0. DO NOT EDIT!
// Options used:
#[allow(dead_code)]
pub mod exports {
    #[allow(dead_code)]
    pub mod yosh {
        #[allow(dead_code)]
        pub mod llm {
            #[allow(dead_code, clippy::all)]
            pub mod llm {
                #[used]
                #[doc(hidden)]
                #[cfg(target_arch = "wasm32")]
                static __FORCE_SECTION_REF: fn() =
                    super::super::super::super::__link_custom_section_describing_imports;
                use super::super::super::super::_rt;
                /// A Large Language Model.
                pub type InferencingModel = _rt::String;
                /// Inference request parameters
                #[repr(C)]
                #[derive(Clone, Copy)]
                pub struct InferencingParams {
                    /// The maximum tokens that should be inferred.
                    ///
                    /// Note: the backing implementation may return less tokens.
                    pub max_tokens: u32,
                    /// The amount the model should avoid repeating tokens.
                    pub repeat_penalty: f32,
                    /// The number of tokens the model should apply the repeat penalty to.
                    pub repeat_penalty_last_n_token_count: u32,
                    /// The randomness with which the next token is selected.
                    pub temperature: f32,
                    /// The number of possible next tokens the model will choose from.
                    pub top_k: u32,
                    /// The probability total of next tokens the model will choose from.
                    pub top_p: f32,
                }
                impl ::core::fmt::Debug for InferencingParams {
                    fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
                        f.debug_struct("InferencingParams")
                            .field("max-tokens", &self.max_tokens)
                            .field("repeat-penalty", &self.repeat_penalty)
                            .field(
                                "repeat-penalty-last-n-token-count",
                                &self.repeat_penalty_last_n_token_count,
                            )
                            .field("temperature", &self.temperature)
                            .field("top-k", &self.top_k)
                            .field("top-p", &self.top_p)
                            .finish()
                    }
                }
                /// The set of errors which may be raised by functions in this interface
                #[derive(Clone)]
                pub enum Error {
                    ModelNotSupported,
                    RuntimeError(_rt::String),
                    InvalidInput(_rt::String),
                }
                impl ::core::fmt::Debug for Error {
                    fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
                        match self {
                            Error::ModelNotSupported => {
                                f.debug_tuple("Error::ModelNotSupported").finish()
                            }
                            Error::RuntimeError(e) => {
                                f.debug_tuple("Error::RuntimeError").field(e).finish()
                            }
                            Error::InvalidInput(e) => {
                                f.debug_tuple("Error::InvalidInput").field(e).finish()
                            }
                        }
                    }
                }
                impl ::core::fmt::Display for Error {
                    fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
                        write!(f, "{:?}", self)
                    }
                }

                impl std::error::Error for Error {}
                /// Usage information related to the inferencing result
                #[repr(C)]
                #[derive(Clone, Copy)]
                pub struct InferencingUsage {
                    /// Number of tokens in the prompt
                    pub prompt_token_count: u32,
                    /// Number of tokens generated by the inferencing operation
                    pub generated_token_count: u32,
                }
                impl ::core::fmt::Debug for InferencingUsage {
                    fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
                        f.debug_struct("InferencingUsage")
                            .field("prompt-token-count", &self.prompt_token_count)
                            .field("generated-token-count", &self.generated_token_count)
                            .finish()
                    }
                }
                /// An inferencing result
                #[derive(Clone)]
                pub struct InferencingResult {
                    /// The text generated by the model
                    /// TODO: this should be a stream
                    pub text: _rt::String,
                    /// Usage information about the inferencing request
                    pub usage: InferencingUsage,
                }
                impl ::core::fmt::Debug for InferencingResult {
                    fn fmt(&self, f: &mut ::core::fmt::Formatter<'_>) -> ::core::fmt::Result {
                        f.debug_struct("InferencingResult")
                            .field("text", &self.text)
                            .field("usage", &self.usage)
                            .finish()
                    }
                }
                #[doc(hidden)]
                #[allow(non_snake_case)]
                pub unsafe fn _export_infer_cabi<T: Guest>(
                    arg0: *mut u8,
                    arg1: usize,
                    arg2: *mut u8,
                    arg3: usize,
                    arg4: i32,
                    arg5: i32,
                    arg6: f32,
                    arg7: i32,
                    arg8: f32,
                    arg9: i32,
                    arg10: f32,
                ) -> *mut u8 {
                    #[cfg(target_arch = "wasm32")]
                    _rt::run_ctors_once();
                    let len0 = arg1;
                    let bytes0 = _rt::Vec::from_raw_parts(arg0.cast(), len0, len0);
                    let len1 = arg3;
                    let bytes1 = _rt::Vec::from_raw_parts(arg2.cast(), len1, len1);
                    let result2 = T::infer(
                        _rt::string_lift(bytes0),
                        _rt::string_lift(bytes1),
                        match arg4 {
                            0 => None,
                            1 => {
                                let e = InferencingParams {
                                    max_tokens: arg5 as u32,
                                    repeat_penalty: arg6,
                                    repeat_penalty_last_n_token_count: arg7 as u32,
                                    temperature: arg8,
                                    top_k: arg9 as u32,
                                    top_p: arg10,
                                };
                                Some(e)
                            }
                            _ => _rt::invalid_enum_discriminant(),
                        },
                    );
                    let ptr3 = _RET_AREA.0.as_mut_ptr().cast::<u8>();
                    match result2 {
                        Ok(e) => {
                            *ptr3.add(0).cast::<u8>() = (0i32) as u8;
                            let InferencingResult {
                                text: text4,
                                usage: usage4,
                            } = e;
                            let vec5 = (text4.into_bytes()).into_boxed_slice();
                            let ptr5 = vec5.as_ptr().cast::<u8>();
                            let len5 = vec5.len();
                            ::core::mem::forget(vec5);
                            *ptr3.add(8).cast::<usize>() = len5;
                            *ptr3.add(4).cast::<*mut u8>() = ptr5.cast_mut();
                            let InferencingUsage {
                                prompt_token_count: prompt_token_count6,
                                generated_token_count: generated_token_count6,
                            } = usage4;
                            *ptr3.add(12).cast::<i32>() = _rt::as_i32(prompt_token_count6);
                            *ptr3.add(16).cast::<i32>() = _rt::as_i32(generated_token_count6);
                        }
                        Err(e) => {
                            *ptr3.add(0).cast::<u8>() = (1i32) as u8;
                            match e {
                                Error::ModelNotSupported => {
                                    *ptr3.add(4).cast::<u8>() = (0i32) as u8;
                                }
                                Error::RuntimeError(e) => {
                                    *ptr3.add(4).cast::<u8>() = (1i32) as u8;
                                    let vec7 = (e.into_bytes()).into_boxed_slice();
                                    let ptr7 = vec7.as_ptr().cast::<u8>();
                                    let len7 = vec7.len();
                                    ::core::mem::forget(vec7);
                                    *ptr3.add(12).cast::<usize>() = len7;
                                    *ptr3.add(8).cast::<*mut u8>() = ptr7.cast_mut();
                                }
                                Error::InvalidInput(e) => {
                                    *ptr3.add(4).cast::<u8>() = (2i32) as u8;
                                    let vec8 = (e.into_bytes()).into_boxed_slice();
                                    let ptr8 = vec8.as_ptr().cast::<u8>();
                                    let len8 = vec8.len();
                                    ::core::mem::forget(vec8);
                                    *ptr3.add(12).cast::<usize>() = len8;
                                    *ptr3.add(8).cast::<*mut u8>() = ptr8.cast_mut();
                                }
                            }
                        }
                    };
                    ptr3
                }
                #[doc(hidden)]
                #[allow(non_snake_case)]
                pub unsafe fn __post_return_infer<T: Guest>(arg0: *mut u8) {
                    let l0 = i32::from(*arg0.add(0).cast::<u8>());
                    match l0 {
                        0 => {
                            let l1 = *arg0.add(4).cast::<*mut u8>();
                            let l2 = *arg0.add(8).cast::<usize>();
                            _rt::cabi_dealloc(l1, l2, 1);
                        }
                        _ => {
                            let l3 = i32::from(*arg0.add(4).cast::<u8>());
                            match l3 {
                                0 => (),
                                1 => {
                                    let l4 = *arg0.add(8).cast::<*mut u8>();
                                    let l5 = *arg0.add(12).cast::<usize>();
                                    _rt::cabi_dealloc(l4, l5, 1);
                                }
                                _ => {
                                    let l6 = *arg0.add(8).cast::<*mut u8>();
                                    let l7 = *arg0.add(12).cast::<usize>();
                                    _rt::cabi_dealloc(l6, l7, 1);
                                }
                            }
                        }
                    }
                }
                pub trait Guest {
                    /// Perform inferencing using the provided model and prompt with the given optional params
                    fn infer(
                        model: InferencingModel,
                        prompt: _rt::String,
                        params: Option<InferencingParams>,
                    ) -> Result<InferencingResult, Error>;
                }
                #[doc(hidden)]

                macro_rules! __export_yosh_llm_llm_cabi{
        ($ty:ident with_types_in $($path_to_types:tt)*) => (const _: () = {

          #[export_name = "yosh:llm/llm#infer"]
          unsafe extern "C" fn export_infer(arg0: *mut u8,arg1: usize,arg2: *mut u8,arg3: usize,arg4: i32,arg5: i32,arg6: f32,arg7: i32,arg8: f32,arg9: i32,arg10: f32,) -> *mut u8 {
            $($path_to_types)*::_export_infer_cabi::<$ty>(arg0, arg1, arg2, arg3, arg4, arg5, arg6, arg7, arg8, arg9, arg10)
          }
          #[export_name = "cabi_post_yosh:llm/llm#infer"]
          unsafe extern "C" fn _post_return_infer(arg0: *mut u8,) {
            $($path_to_types)*::__post_return_infer::<$ty>(arg0)
          }
        };);
      }
                #[doc(hidden)]
                pub(crate) use __export_yosh_llm_llm_cabi;
                #[repr(align(4))]
                struct _RetArea([::core::mem::MaybeUninit<u8>; 20]);
                static mut _RET_AREA: _RetArea = _RetArea([::core::mem::MaybeUninit::uninit(); 20]);
            }
        }
    }
}
mod _rt {
    pub use alloc_crate::string::String;

    #[cfg(target_arch = "wasm32")]
    pub fn run_ctors_once() {
        wit_bindgen_rt::run_ctors_once();
    }
    pub use alloc_crate::vec::Vec;
    pub unsafe fn string_lift(bytes: Vec<u8>) -> String {
        if cfg!(debug_assertions) {
            String::from_utf8(bytes).unwrap()
        } else {
            String::from_utf8_unchecked(bytes)
        }
    }
    pub unsafe fn invalid_enum_discriminant<T>() -> T {
        if cfg!(debug_assertions) {
            panic!("invalid enum discriminant")
        } else {
            core::hint::unreachable_unchecked()
        }
    }

    pub fn as_i32<T: AsI32>(t: T) -> i32 {
        t.as_i32()
    }

    pub trait AsI32 {
        fn as_i32(self) -> i32;
    }

    impl<'a, T: Copy + AsI32> AsI32 for &'a T {
        fn as_i32(self) -> i32 {
            (*self).as_i32()
        }
    }

    impl AsI32 for i32 {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for u32 {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for i16 {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for u16 {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for i8 {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for u8 {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for char {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }

    impl AsI32 for usize {
        #[inline]
        fn as_i32(self) -> i32 {
            self as i32
        }
    }
    pub unsafe fn cabi_dealloc(ptr: *mut u8, size: usize, align: usize) {
        if size == 0 {
            return;
        }
        let layout = alloc::Layout::from_size_align_unchecked(size, align);
        alloc::dealloc(ptr as *mut u8, layout);
    }
    extern crate alloc as alloc_crate;
    pub use alloc_crate::alloc;
}

/// Generates `#[no_mangle]` functions to export the specified type as the
/// root implementation of all generated traits.
///
/// For more information see the documentation of `wit_bindgen::generate!`.
///
/// ```rust
/// # macro_rules! export{ ($($t:tt)*) => (); }
/// # trait Guest {}
/// struct MyType;
///
/// impl Guest for MyType {
///     // ...
/// }
///
/// export!(MyType);
/// ```
#[allow(unused_macros)]
#[doc(hidden)]

macro_rules! __export_w_impl {
  ($ty:ident) => (self::export!($ty with_types_in self););
  ($ty:ident with_types_in $($path_to_types_root:tt)*) => (
  $($path_to_types_root)*::exports::yosh::llm::llm::__export_yosh_llm_llm_cabi!($ty with_types_in $($path_to_types_root)*::exports::yosh::llm::llm);
  )
}
#[doc(inline)]
pub(crate) use __export_w_impl as export;

#[cfg(target_arch = "wasm32")]
#[link_section = "component-type:wit-bindgen:0.25.0:w:encoded world"]
#[doc(hidden)]
pub static __WIT_BINDGEN_COMPONENT_TYPE: [u8; 648] = *b"\
\0asm\x0d\0\x01\0\0\x19\x16wit-component-encoding\x04\0\x07\x90\x04\x01A\x02\x01\
A\x02\x01B\x16\x01s\x04\0\x11inferencing-model\x03\0\0\x01r\x06\x0amax-tokensy\x0e\
repeat-penaltyv!repeat-penalty-last-n-token-county\x0btemperaturev\x05top-ky\x05\
top-pv\x04\0\x12inferencing-params\x03\0\x02\x01q\x03\x13model-not-supported\0\0\
\x0druntime-error\x01s\0\x0dinvalid-input\x01s\0\x04\0\x05error\x03\0\x04\x01r\x02\
\x12prompt-token-county\x15generated-token-county\x04\0\x11inferencing-usage\x03\
\0\x06\x01r\x02\x04texts\x05usage\x07\x04\0\x12inferencing-result\x03\0\x08\x01s\
\x04\0\x0fembedding-model\x03\0\x0a\x01r\x01\x12prompt-token-county\x04\0\x10emb\
eddings-usage\x03\0\x0c\x01pv\x01p\x0e\x01r\x02\x0aembeddings\x0f\x05usage\x0d\x04\
\0\x11embeddings-result\x03\0\x10\x01k\x03\x01j\x01\x09\x01\x05\x01@\x03\x05mode\
l\x01\x06prompts\x06params\x12\0\x13\x04\0\x05infer\x01\x14\x04\x01\x0cyosh:llm/\
llm\x05\0\x04\x01\x0ayosh:llm/w\x04\0\x0b\x07\x01\0\x01w\x03\0\0\0G\x09producers\
\x01\x0cprocessed-by\x02\x0dwit-component\x070.208.1\x10wit-bindgen-rust\x060.25\
.0";

#[inline(never)]
#[doc(hidden)]
#[cfg(target_arch = "wasm32")]
pub fn __link_custom_section_describing_imports() {
    wit_bindgen_rt::maybe_link_cabi_realloc();
}
